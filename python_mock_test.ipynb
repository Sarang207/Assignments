{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3344a688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002,2009,2016,2023,2037,2044,2051,2058,2072,2079,2086,2093,2107,2114,2121,2128,2142,2149,2156,2163,2177,2184,2191,2198,2212,2219,2226,2233,2247,2254,2261,2268,2282,2289,2296,2303,2317,2324,2331,2338,2352,2359,2366,2373,2387,2394,2401,2408,2422,2429,2436,2443,2457,2464,2471,2478,2492,2499,2506,2513,2527,2534,2541,2548,2562,2569,2576,2583,2597,2604,2611,2618,2632,2639,2646,2653,2667,2674,2681,2688,2702,2709,2716,2723,2737,2744,2751,2758,2772,2779,2786,2793,2807,2814,2821,2828,2842,2849,2856,2863,2877,2884,2891,2898,2912,2919,2926,2933,2947,2954,2961,2968,2982,2989,2996,3003,3017,3024,3031,3038,3052,3059,3066,3073,3087,3094,3101,3108,3122,3129,3136,3143,3157,3164,3171,3178,3192,3199\n"
     ]
    }
   ],
   "source": [
    "#Que : 1\n",
    "a = [str(num) for num in range(2000, 3201) if num % 7 == 0 and num % 5 != 0]\n",
    "print(','.join(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9b0cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: My school days are best ever\n",
      "Number of uppercase letters: 1\n",
      "Number of lowercase letters: 22\n"
     ]
    }
   ],
   "source": [
    "#Que : 2\n",
    "s = input(\"Enter a sentence: \")\n",
    "\n",
    "u_count = sum(1 for char in s if char.isupper())\n",
    "l_count = sum(1 for char in s if char.islower())\n",
    "\n",
    "print(\"Number of uppercase letters:\", u_count)\n",
    "print(\"Number of lowercase letters:\", l_count)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74896e9a",
   "metadata": {},
   "source": [
    "#Que : 3\n",
    "Given a large dataset with 1 million rows and 1000 columns for a classification problem, \n",
    "the goal is to reduce dimensionality to speed up model computation, considering memory constraints. \n",
    "\n",
    "a simplified approach:\n",
    "\n",
    "Use Dimensionality Reduction Techniques: Employ techniques like Principal Component Analysis (PCA) \n",
    "to reduce the number of features while retaining the most important information. \n",
    "PCA transforms the original features into a lower-dimensional space, preserving the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa943797",
   "metadata": {},
   "source": [
    "#Que : 4\n",
    "While getting a 96% accuracy in a cancer detection model is good,\n",
    "1. Imbalance in Data: If there are more cases of non-cancer than cancer in the dataset,\n",
    "the model might just predict the more common class to achieve high accuracy. \n",
    "\n",
    "conclusion,  high accuracy is positive, it's crucial to consider other factors and work closely with domain\n",
    "experts to build a reliable machine learning model for cancer detection.\n",
    "\n",
    "1. Prior Probability (P(A)):\n",
    "In Naive Bayes, it refers to the probability of a particular class (A) before observing any features.\n",
    "It's denoted as P(A).\n",
    "\n",
    "2. Likelihood (P(X|A)):\n",
    "\n",
    "The likelihood is the probability of observing the given set of features (X) given a particular class (A).\n",
    "In Naive Bayes, the assumption is that features are conditionally independent given the class. \n",
    "This simplifies the computation of the likelihood.\n",
    "\n",
    "3. Marginal Likelihood (P(X)):\n",
    "In Naive Bayes, it's calculated as the sum of the joint probabilities of each class and the given set of features.\n",
    "It serves as a normalizing factor."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7877796e",
   "metadata": {},
   "source": [
    "#Que 5:\n",
    "Yes, it is entirely possible and not uncommon for  regression models to outperform \n",
    "decision tree models in  time series forecasting tasks.\n",
    "\n",
    "  \n",
    "1.Sensitivity to noise:  Decision trees tend to overfit, especially when the data set is noisy or has a lot of variation.\n",
    "#    regression models may generalize better by capturing underlying trends and patterns.\n",
    " \n",
    "2. Temporal dependence:  Time series data often exhibit temporal dependencies, and regression models can  account\n",
    "    for these dependencies explicitly.\n",
    " \n",
    " 3. Continuous prediction:  Decision trees make categorical predictions based on the majority class of leaf nodes.\n",
    " In contrast, regression models provide continuous predictions and therefore may be more suitable for tasks where \n",
    "the target variable is numerical and continuously changing.\n",
    " \n",
    " 4. Feature importance:  Decision trees provide a measure of feature importance, whereas regression models often quantify \n",
    "how each feature contributes to prediction.\n",
    "\n",
    "Provides a simpler interpretation of what to do.this is important for understanding the dynamics of time series"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b951b44d",
   "metadata": {},
   "source": [
    "#Que 6 :\n",
    "To handle low bias and high variance, use ensemble techniques such as random forests.\n",
    " Random forests create multiple decision trees to reduce overfitting and improve generalization.\n",
    " Built-in regularization, robustness to noisy data, insight into feature importance, parallelization,\n",
    "and tunable hyperparameters make it a good choice.\n",
    " To further improve performance, consider other ensemble methods such as gradient boosting.\n",
    " Validate your model with a separate test set and use cross-validation for robust evaluation.\n",
    " Choose an algorithm based on the characteristics of your dataset  and your modeling goals."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b646c7b",
   "metadata": {},
   "source": [
    "#Que 9 :\n",
    "Ridge and Lasso Regression:  - Use Ridge Regression when multicollinearity is a concern, \n",
    "    or when it is desirable to do some regularization and retain all variables.\n",
    " Selection of important variables: \n",
    "    Use tree feature importance  (Random Forest, Gradient Boosting).\n",
    " – Analyze the correlation matrix and select variables that are highly correlated with the target.\n",
    " - Apply statistical tests (such as p-value threshold analysis) for the significance of variables.\n",
    " - Use Lasso regression or Elastic Net for sparse model selection.\n",
    " – Consider domain knowledge and expert opinion about the meaning of variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491e39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Que 12 :\n",
    "def create_square_dictionary():\n",
    "    square_dict = {num: num**2 for num in range(1, 21)}\n",
    "    return square_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae3bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81, 10: 100, 11: 121, 12: 144, 13: 169, 14: 196, 15: 225, 16: 256, 17: 289, 18: 324, 19: 361, 20: 400}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r_dict = create_square_dictionary()\n",
    "print(r_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae70233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Half: (1, 2, 3, 4, 5)\n",
      "Last Half: (6, 7, 8, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "#Que 13 :\n",
    "tup = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "\n",
    "print(\"First Half:\", tup[:len(tup)//2])\n",
    "print(\"Last Half:\", tup[len(tup)//2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d58310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Que 14 :\n",
    "import random\n",
    "\n",
    "def generate_even_numbers():\n",
    "    even_numbers = random.sample(range(100, 201, 2), 5)\n",
    "    return even_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c40a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Even Numbers: [186, 172, 154, 126, 164]\n"
     ]
    }
   ],
   "source": [
    "list1 = generate_even_numbers()\n",
    "print(\"Random Even Numbers:\", list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fb4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
